"Explain the concept of Parameter-Efficient Fine-Tuning (PEFT).","PEFT is a method that reduces the number of trainable parameters needed to adapt a large pretrained model to specific downstream applications, addressing the challenges of full fine-tuning."
"What are the advantages of Parameter-Efficient Fine-Tuning (PEFT)?","PEFT decreases computational resources and memory storage needed and offers more stability than full fine-tuning, especially for NLP use cases."
"Describe Supervised Fine-Tuning (SFT).","SFT is a method used in machine learning to adapt a model's pretrained knowledge to a new task, involving updates to learning parameters, layers, and neurons of large language models."
"What are the challenges of full fine-tuning in Supervised Fine-Tuning (SFT)?","Full fine-tuning requires significant computational resources and memory, substantial task-specific labeled data, and has a higher risk of overfitting, being time-consuming and complex in implementation."
"What is catastrophic forgetting in the context of SFT?","Catastrophic forgetting occurs when a model forgets previously learned information when trained with new data."
"Name the types of PEFT methods discussed.","Selective Fine-Tuning, Additive Fine-Tuning, Reparameterization Fine-Tuning."
"What is Selective Fine-Tuning?","Selective Fine-Tuning updates only a subset of layers or parameters, although it's less effective for transformer architectures."
"Explain Additive Fine-Tuning.","Additive Fine-Tuning adds new task-specific layers or components to the pretrained model, using adapters for transformers to allow task-specific customization while preserving pretrained knowledge."
"What is Reparameterization Fine-Tuning and give an example method.","Reparameterization Fine-Tuning uses low-rank transformations to reparameterize network weights, reducing trainable parameters. An example method is LoRA (Low-Rank Adaptation)."
"Describe LoRA in the context of Reparameterization Fine-Tuning.","LoRA adds low-rank layers to the original layer, capturing important data directions, maintaining model performance while reducing computational costs."
"What are soft prompts?","Soft prompts are learnable tensors concatenated with input embeddings, optimized for specific datasets, including methods like Prompt tuning, Prefix tuning, P-tuning, and Multitask prompt tuning."
"Explain Prefix Tuning as a method of soft prompts.","In Prefix Tuning, instead of training the entire model, parameter embeddings are appended to existing embeddings, and all parameters except for the embeddings are frozen during training."
"What is the concept of rank in neural networks?","Rank indicates the minimum number of vectors needed to span a space, similar to the concept of dimension, and in neural networks, low-rank operations can reduce the number of parameters, making the model more efficient."
"Name some Reparameterization-Based Methods discussed.","LoRA, QLoRA, DoRA."
"What is QLoRA?","QLoRA combines low-rank adaptations with quantization to reduce memory and computational needs."
"What is DoRA?","DoRA adjusts rank in low-rank space based on component magnitude for optimized performance and efficiency."
"Why are PEFT methods essential?","PEFT methods are essential for efficiently adapting large pretrained models to new tasks while minimizing computational requirements and preserving pretrained knowledge."