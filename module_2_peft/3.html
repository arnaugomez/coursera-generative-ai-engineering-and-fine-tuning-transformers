<html><head>
                <style>
                    .linenums {
                        list-style-type: none;
                    }

                    .formatted-line-numbers {
                        display: none;
                    }
                    .action-code-block {
                        display: none;
                    }
                    table {
                        border-collapse: collapse;
                        width: 100%;
                    }
                    table, th, td {
                        border: 1px solid black;
                        padding: 8px;
                        text-align: left;
                    }
                </style>
            </head><body><div> 
    
<h1><span class="header-link octicon octicon-link"></span>Glossary: Generative AI Engineering and Fine-Tuning Transformers</h1><div>

<p>Welcome! This alphabetized glossary contains many terms used in this course. Understanding these terms is essential when working in the industry, participating in user groups, and participating in other certificate programs.</p>
<p>Estimated reading time: 5 minutes</p>
<table>
<thead>
<tr>
<th>Term</th>
<th>Definition</th>
</tr>
</thead>
<tbody><tr>
<td>AdamW optimizer</td>
<td>A stochastic optimization method that helps in modifying the implementation of weight decay from gradient update.</td>
</tr>
<tr>
<td>Additive fine-tuning</td>
<td>A method that involves adding new task-specific layers or components to the pre-trained model.</td>
</tr>
<tr>
<td>AG News dataset</td>
<td>A subdataset of AG corpus of news articles.</td>
</tr>
<tr>
<td>Bidirectional representation of transformers (BERT)</td>
<td>An open-source model that offers deeply bidirectional, unsupervised language representations, pretrained on a plain text corpus.</td>
</tr>
<tr>
<td>Bidirectional representation of transformers (BERT) tokenizer</td>
<td>An important component for processing input data before inserting it into the BERT model.</td>
</tr>
<tr>
<td>ChatGPT</td>
<td>An artificial chatbot developed by OpenAI.</td>
</tr>
<tr>
<td>Contextual embeddings</td>
<td>A type of embedding that aptly describes how the transformer processes the input word embeddings by accounting for the context in which each word occurs within the sequence.</td>
</tr>
<tr>
<td>Data leakage</td>
<td>An organization faces challenges in exposing sensitive information.</td>
</tr>
<tr>
<td>Data loader</td>
<td>A utility in a machine learning framework that collects operational data from data sources at regular intervals.</td>
</tr>
<tr>
<td>Datapoint</td>
<td>An identifiable element in the dataset.</td>
</tr>
<tr>
<td>Direct preference optimization (DPO)</td>
<td>A successful fine-tuning strategy for aligning large language models (LLMs) with human preferences without training a reward model or employing reinforcement learning.</td>
</tr>
<tr>
<td>Fine-tuning</td>
<td>A supervised process that optimizes the initially trained GPT model for specific tasks, like QA classification.</td>
</tr>
<tr>
<td>Generative pre-trained transformer (GPT)</td>
<td>A self-supervised model that involves training a decoder to predict the subsequent token or word in a sequence.</td>
</tr>
<tr>
<td>GitHub</td>
<td>A developer platform to create, store, manage, and share codes.</td>
</tr>
<tr>
<td>Global vector for word representation (GloVe) dataset</td>
<td>An unsupervised learning algorithm for obtaining vector representation for word.</td>
</tr>
<tr>
<td>Graphic processing unit (GPU)</td>
<td>A process that helps to render graphic smoothly.</td>
</tr>
<tr>
<td>Hugging Face</td>
<td>A platform that offers an open-source library with pretrained models and tools to streamline the process of training and fine-tuning generative AI models.</td>
</tr>
<tr>
<td>IMBD dataset</td>
<td>A collection of information about movies, TV shows, and video games.</td>
</tr>
<tr>
<td>LangChain</td>
<td>An open-source interface that simplifies the application development process using LLMs. It facilitates a structured way to integrate language models into various use cases, including natural language processing or NLP and data retrieval.</td>
</tr>
<tr>
<td>LangChain â€“ Core</td>
<td>A LangChain Expression Language is the base for abstractions.</td>
</tr>
<tr>
<td>LangChain community</td>
<td>LangChain community is a third-party integrations that implement the base interfaces defined in LangChain Core, making them ready for use in any LangChain application.</td>
</tr>
<tr>
<td>Large language models (LLMs)</td>
<td>Foundation models that use AI and deep learning with vast datasets to generate text, translate languages, and create various types of content. They are called large language models due to the size of the training dataset and the number of parameters.</td>
</tr>
<tr>
<td>Llama</td>
<td>A large language model (LLM) trained by Meta AI understands and responds to human input and generates human-like text.</td>
</tr>
<tr>
<td>Low-rank adaptation (LoRA)</td>
<td>A technique that quickly adapts machine learning models.</td>
</tr>
<tr>
<td>Low-rank transformations</td>
<td>Techniques that approximate large matrices by smaller matrices in order to make computations more efficient.</td>
</tr>
<tr>
<td>Machine learning</td>
<td>Machine learning is a data analysis method for automating analytical model building.</td>
</tr>
<tr>
<td>Natural language processing (NLP)</td>
<td>The subfield of artificial intelligence (AI) deals with the interaction of computers and humans in human language. It involves creating algorithms and models that will help computers understand and comprehend human language and generate contextually relevant text in human language.</td>
</tr>
<tr>
<td>Neural network</td>
<td>Computational models inspired by the structure of the human brain. A neural network model comprises an input layer, one or more hidden layers, and an output layer.</td>
</tr>
<tr>
<td>Parameter-efficient fine-tuning (PEFT)</td>
<td>A method that adapts large pre-trained language models for new tasks with less computational costs.</td>
</tr>
<tr>
<td>Prefix tuning</td>
<td>The prefix tuning adds a sequence of learnable embeddings to the key and value vectors of the attention mechanism in each transformer layer.</td>
</tr>
<tr>
<td>Prompt injection</td>
<td>Prompt injection inserts task-specific tokens or embeddings into the input text at multiple positions.</td>
</tr>
<tr>
<td>Prompt tuning</td>
<td>Prompt tuning is a set of continuous prompt embeddings that are pretended to the input text.</td>
</tr>
<tr>
<td>P-tuning</td>
<td>P-tuning incorporates learnable prompt embeddings directly into the input embedding space that are task-specific and learned during fine-tuning.</td>
</tr>
<tr>
<td>Python</td>
<td>A programming language.</td>
</tr>
<tr>
<td>PyTorch</td>
<td>A software-based open-source deep learning framework used to build neural networks, combining Torch's machine learning library with a Python-based high-level API.</td>
</tr>
<tr>
<td>PyTorch tokenizer</td>
<td>Converts character strings into tokens understood by different PyTorch models.</td>
</tr>
<tr>
<td>Quantized low-rank adaptation  (QLoRA)</td>
<td>Combines low-rank adaptation with quantization, reducing the model's memory footprint and computational requirements.</td>
</tr>
<tr>
<td>Reinforcement learning from human feedback (RLHF)</td>
<td>A model that represents a fine-tuning approach and enhances model performance on specific tasks, proving particularly effective in chatbot development.</td>
</tr>
<tr>
<td>Reparameterization-based methods</td>
<td>Leverage the concept of reparametrizing network weights using low-rank transformations.</td>
</tr>
<tr>
<td>Selective fine-tuning</td>
<td>Updates only a subset of layers or parameters and works for other networks.</td>
</tr>
<tr>
<td>Soft prompts</td>
<td>Soft prompts are an advanced concept that involves modification in the input data to guide the pre-trained language models to achieve desired outputs.</td>
</tr>
<tr>
<td>Supervised fine-tuning (SFT)</td>
<td>A method commonly used in machine learning, especially when working with pre-trained models in transfer learning.</td>
</tr>
<tr>
<td>Supervised fine-tuning (SFT) trainer</td>
<td>A technique to enhance the performance of pre-trained AI agents.</td>
</tr>
<tr>
<td>Transformer model</td>
<td>A model that can translate text and speech in near real-time.</td>
</tr>
<tr>
<td>Tokenization</td>
<td>The process of converting the words in the prompt into tokens.</td>
</tr>
<tr>
<td>Vector</td>
<td>A mathematical object represented by a group of numbers commonly used in machine learning algorithms.</td>
</tr>
<tr>
<td>Weight-decomposed low-rank adaptation (DoRA)</td>
<td>Adjusts the rank of the low-rank space based on the magnitude of the components, optimizing the model's performance and efficiency.</td>
</tr>
</tbody></table>
<p><img src="https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/ZzqKzMYvDxlItsE7xSlAXw.png" alt=""></p>


</div></div></body></html>